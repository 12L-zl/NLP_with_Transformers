{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1fa81e",
   "metadata": {},
   "source": [
    "### 대규모 데이터셋 수집하기\n",
    "#### 대규모 말뭉치 구축의 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf4bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0052fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90775868ec214c859e374563f8f47e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knuyh\\.cache\\huggingface\\hub\\models--openai-gpt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7030eb92c84537bb24695bb9f9a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fabd48e7364d3f8c02d7cc087fbf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d471b0df37f546ae84714338d6ab89a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecbb9eb09e548d0aeb6bd6c2ea8673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7b69ab4d2c4d1d8394f16554727da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd46af2f29f4308b4dc8339608dc90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a465aa7924e46d9ab2631ae258bc537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knuyh\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc4c0a901784ef89c465b52cdfc7bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f00f3fbbc794b1e93aa32a09a05987f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90d3c3739994ddebc4195190d54a24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0fdb2fa88641c2a549140d03b33083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370be002a4bc401bab94453703031d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cbdbedce1045cd9a64060d40fe83e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPT vs GPT-2 비교\n",
    "# GPT : BookCorpus에서 훈련\n",
    "# GPT-2 : 웹페이지, 블로그, 레딧에 링크된 뉴스 기사에서 훈련\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline('text-generation', model = 'openai-gpt')\n",
    "generation_gpt2 = pipeline('text-generation', model = 'gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd5337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT  크기: 116.5M parameters\n",
      "GPT2 크기: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 개수 출력\n",
    "def model_size(model) :\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"GPT  크기: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 크기: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc5afa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 자동 완성:\n",
      "1.\n",
      "When they came back to the tent. \" \n",
      " \" i 'll make sure you have a nice meal, \" said gregor, feeling tired enough to pass out again. \" i'm sorry i had to be so rude in your world. \" \n",
      " \"\n",
      "2.\n",
      "When they came back to the room after my run. \" \n",
      " \" what's this a house? \" \n",
      " \" it's big. but it's not full. it only holds about four people. i have a sister. \" \n",
      " \" her name\n",
      "3.\n",
      "When they came back. when they came up to the room. i knew if they were there i would go see my uncle. he always wanted to talk to me. and i had to answer the door. but he didn't take a step toward\n",
      "\n",
      "GPT-2 자동 완성:\n",
      "1.\n",
      "When they came back, the family moved.\n",
      "\n",
      "Her stepmother and step-grandmother, who had never met each other, had never met either of them, she told The Daily Beast.\n",
      "\n",
      "\"She told me, 'M\n",
      "2.\n",
      "When they came back out of the woods, they found them carrying an old man from their home, and the man they had taken in as a friend for a while had already died.\n",
      "\n",
      "So they returned back to the house, and the\n",
      "3.\n",
      "When they came back from China they knew what time the end-of-year ceremonies were and there hadn't been much action since 2011 but they knew that this time the celebration was off when the time was right.\n",
      "\n",
      "And that was the\n"
     ]
    }
   ],
   "source": [
    "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
    "               clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT 자동 완성:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT-2 자동 완성:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6683d1b",
   "metadata": {},
   "source": [
    "GPT : 로맨스로 편향된 부분 있음, 남자와 여자의 낭만적인 대화로 보임  \n",
    "GPT-2 : 레딧 기사에 연결된 웹 텍스트에서 훈련되어 블로그 같은 내용이나 모험적인 요소를 담고 있고 중립적인 they 사용  \n",
    "\n",
    "* 구글 빅쿼리로 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6ec99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d489585530b8430f8445fa2c6e05eae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06998d6ef044f6695b0246e4ce263ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\",\n",
    "                              streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50767c81",
   "metadata": {},
   "source": [
    "### 토크나이저 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6466c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbb9e019f3845729aa659d761ea6dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knuyh\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb309dcfe9944aaca8709da982e71ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d654649be99414787a324b8903eac68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f4eaeaccc446f1a5dc06293d034794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knuyh\\.cache\\huggingface\\hub\\models--camembert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f22f548d874b1fb1874def7b470203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc66e6be77594900a639c9cc3b153c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07f92fc42c049e58d7015b2d4d66eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5755d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"sex\"에 대한 T5 토큰: ['', 's', 'ex']\n",
      "\"being\"에 대한 CamemBERT 토큰: ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(f'\"sex\"에 대한 T5 토큰: {tok_list(tokenizer_T5,\"sex\")}')\n",
    "print(f'\"being\"에 대한 CamemBERT 토큰: {tok_list(tokenizer_camembert,\"being\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee98a6",
   "metadata": {},
   "source": [
    "짧고, 평범한 단어를 부분으로 나누면 모델에 입력되는 시퀀스 길이가 늘어나 비효율적  \n",
    "\n",
    "* 토크나이저 성능 측정하기\n",
    "    * 부분단어 생산력 : 토큰화된 단어마다 생성되는 부분단어의 평균 개수 계산\n",
    "    * 연속 단어의 비율 : 말뭉치에서 적어도 두 개의 부분 토큰으로 분할된 토큰화된 단어의 비율\n",
    "    * 커버리지 측정값 : 토큰화된 말뭉치에서 알 수 없는 단어나 거의 사용되지 않는 토큰의 비율\n",
    "    \n",
    "* 파이썬 코드를 위한 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574acb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "python_code = r\"\"\"def say_hello():\n",
    "    print(\"Hello, World!\")\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5087638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer)\n",
    "# GPT-2는 정규화 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c1c6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello', (28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72246d",
   "metadata": {},
   "source": [
    "토크나이저는 문자열과 토큰 사이를 전환하는 데 매우 유용한 오프셋트래킹 기능이 있다. 입력 문자열에 대한 모든 연산이 추적되기에 토큰화 이후 토큰이 입력 문자열의 어떤 부분에 해당하는지 정확하게 알 수 있다.  \n",
    "\n",
    "숫자는 단순히 토큰이 유래된 원본 문자열의 위치로, 일부 문자가 정규화 단계에서 삭제되더라도 각 토큰을 원본 문자열의 해당 부분에 연결 가능  \n",
    "기이한 문자는 유니코드 문자를 바이트의 시퀀스로 변환하여 생긴 것  \n",
    "(`Ċ` : 줄바꿈 , `Ġ` : 공백)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "063a9d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`a`는 단일 바이트 `b'a'`로 인코딩됩니다: 97\n",
      "`€`는 세 바이트 `b'\\xe2\\x82\\xac'`로 인코딩됩니다: [226, 130, 172]\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f'`{a}`는 단일 바이트 `{a.encode(\"utf-8\")}`로 인코딩됩니다: {byte}')\n",
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
    "print(f'`{e}`는 세 바이트 `{e.encode(\"utf-8\")}`로 인코딩됩니다: {byte}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624d4870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 어휘 사전 크기: 256\n",
      "첫 번째 원소: `!`, last element: `Ń`\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f'기본 어휘 사전 크기: {len(base_vocab)}')\n",
    "print(f'첫 번째 원소: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc1f72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Character</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Mapped bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regular characters</td>\n",
       "      <td>`a` and `?`</td>\n",
       "      <td>97 and 63</td>\n",
       "      <td>`a` and `?`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nonprintable control character (carriage return)</td>\n",
       "      <td>`U+000D`</td>\n",
       "      <td>13</td>\n",
       "      <td>`č`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A space</td>\n",
       "      <td>` `</td>\n",
       "      <td>32</td>\n",
       "      <td>`Ġ`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A nonbreakable space</td>\n",
       "      <td>`\\xa0`</td>\n",
       "      <td>160</td>\n",
       "      <td>`ł`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A newline character</td>\n",
       "      <td>`\\n`</td>\n",
       "      <td>10</td>\n",
       "      <td>`Ċ`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Description    Character      Bytes  \\\n",
       "0                                Regular characters  `a` and `?`  97 and 63   \n",
       "1  Nonprintable control character (carriage return)     `U+000D`         13   \n",
       "2                                           A space          ` `         32   \n",
       "3                              A nonbreakable space       `\\xa0`        160   \n",
       "4                               A newline character         `\\n`         10   \n",
       "\n",
       "  Mapped bytes  \n",
       "0  `a` and `?`  \n",
       "1          `č`  \n",
       "2          `Ġ`  \n",
       "3          `ł`  \n",
       "4          `Ċ`  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE 문자 매핑의 예\n",
    "import pandas as pd\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "examples = [\n",
    "    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n",
    "    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n",
    "    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n",
    "    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n",
    "    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n",
    "]\n",
    "\n",
    "pd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb1fe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello', (28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b79761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"어휘 사전의 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a9f3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f41ba",
   "metadata": {},
   "source": [
    "* 토크나이저 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2447e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', ' =================================================================', ' ----------------------------------------------------------------', '________________________________________________________________', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', '----------------------------------------------------------------', '================================================================', '................................................................']\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 어휘사전에서 가장 긴 단어\n",
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a5a96d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated', ' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
     ]
    }
   ],
   "source": [
    "# 어휘사전의 마지막에 등록된 단어\n",
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b834374",
   "metadata": {},
   "source": [
    "<|endoftext|> : 텍스트 시퀀스의 끝 지정할 때 사용하는 특수 토큰으로, BPE 어휘사전이 구축된 후 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "length = 100000\n",
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n",
    "                                                  vocab_size=12500,\n",
    "                                                  initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE 알고리즘이 만든 첫 단어\n",
    "# 256바이트는 건너뛰고 그다음에 추가된 첫 토큰\n",
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[257:280]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ececed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 단어\n",
    "print([f'{new_tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7077e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df538b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword\n",
    "\n",
    "print(f'파이썬 전체 예약어 개수: {len(keyword.kwlist)}')\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f'예약어 `{keyw}`는 어휘 사전에 없습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974471c",
   "metadata": {},
   "source": [
    "finally 같이 매우 자주 등장하는 예약어가 어휘사전에 없다. 데이터셋에서 더 많은 샘플을 가져와 더 큰 어휘사전을 만들어야."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n",
    "    vocab_size=32768, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 토큰\n",
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n",
    "                reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())\n",
    "# 들여쓰기가 어휘사전에 유지됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f'예약어 `{keyw}`는 어휘 사전에 없습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234c665",
   "metadata": {},
   "source": [
    "드문 단어라 어휘사전에 없는 것이 합당함\n",
    "\n",
    "### 밑바닥부터 모델 훈련하기\n",
    "#### 사전 훈련 목표\n",
    "* 코잘 언어 모델링\n",
    "    * 코드 샘플 시작 부분을 모델에게 제공하고 코드의 나머지 부분을 생성해 완성하라고 요청하는 것\n",
    "    * 레이블이 없는 데이터셋을 사용하는 자기 지도 훈련 목표\n",
    "* 마스크드 언어 모델링\n",
    "    * 모델에게 잡음이 섞인 코드 샘플을 주고 깨끗한 원본 샘플을 재구성하라고 요청하는 자기 지도 훈련 목표\n",
    "* 시퀀스-투-시퀀스 훈련\n",
    "    * 정규식 같은 수동 규칙으로 주석이나 독스트링을 코드에서 분리해 레이블링된 데이터셋으로 사용하도록 (코드, 주석) 쌍의 대규모 데이터셋을 구축하는 작업\n",
    "    \n",
    "#### 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
    "config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4928843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPT-2 (xl) 크기: {model_size(model)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbf968",
   "metadata": {},
   "source": [
    "#### 데이터로더 구축하기\n",
    "시퀀스 마지막 부분을 많이 잃지 않도록, 여러 샘플을 토큰화한 다음 특수한 EOS 토큰으로 연결해 매우 긴 시퀀스를 만든다. 이 시퀀스를 동일한 크기의 청크로 나누면 마지막 데이터에서 손실되는 부분이 미미하다.\n",
    "\n",
    "<br>\n",
    "\n",
    "input_characters = number_of_sequences * sequence_length * characters_per_token  \n",
    "* input_characters : 토크나이저에 입력된 문자열에 있는 문자의 개수\n",
    "* number_of_sequences : 토크나이저로부터 얻으려는 시퀀스의 개수\n",
    "* sequence_length : 토크나이저가 반환한 각 시퀀스의 토큰 개수\n",
    "* characters_per_token : 사전에 추정해야 하는 각 출력 토큰의 평균 문자 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc11838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰의 평균 문자 길이\n",
    "examples, total_characters, total_tokens = 500, 0, 0\n",
    "dataset = load_dataset('transformersbook/codeparrot-train', split='train',\n",
    "                       streaming=True)\n",
    "\n",
    "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "    total_characters += len(example['content'])\n",
    "    total_tokens += len(tokenizer(example['content']).tokens())\n",
    "\n",
    "characters_per_token = total_characters / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(characters_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
    "                 num_of_sequences=1024, chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "\n",
    "            all_token_ids = []\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_inputs['input_ids']:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            \n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n",
    "                                                num_of_sequences=10)\n",
    "dataset_iterator = iter(constant_length_dataset)\n",
    "\n",
    "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
    "print(f\"시퀀스 길이: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8109ac",
   "metadata": {},
   "source": [
    "#### 훈련 루프 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# 작은 모델에 해당하는 파라미터\n",
    "config = {\"train_batch_size\": 2, # 12\n",
    "          \"valid_batch_size\": 2, # 12\n",
    "          \"weight_decay\": 0.1,\n",
    "          \"shuffle_buffer\": 1000,\n",
    "          \"learning_rate\": 2e-4, # 5e-4\n",
    "          \"lr_scheduler_type\": \"cosine\",\n",
    "          \"num_warmup_steps\": 750, # 2000\n",
    "          \"gradient_accumulation_steps\": 16, # 1\n",
    "          \"max_train_steps\": 50000, # 150000\n",
    "          \"max_eval_steps\": -1,\n",
    "          \"seq_length\": 1024,\n",
    "          \"seed\": 1,\n",
    "          \"save_checkpoint_steps\": 50000} # 15000\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "def setup_logging(project_name):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n",
    "        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
    "        logging.StreamHandler()])\n",
    "    if accelerator.is_main_process: # 로깅 한 번만 설정\n",
    "        wandb.init(project=project_name, config=args)\n",
    "        run_name = wandb.run.name\n",
    "        tb_writer = SummaryWriter()\n",
    "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
    "        logger.setLevel(logging.INFO)\n",
    "        datasets.utils.logging.set_verbosity_debug()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        run_name = ''\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    return logger, tb_writer, run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(step, metrics):\n",
    "    logger.info(f\"Step {step}: {metrics}\")\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log(metrics)\n",
    "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def create_dataloaders(dataset_name):\n",
    "    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n",
    "                              streaming=True)\n",
    "    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n",
    "                                    seed=args.seed)\n",
    "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n",
    "                              streaming=True)\n",
    "    \n",
    "    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n",
    "                                          seq_length=args.seq_length)\n",
    "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\n",
    "                                          seq_length=args.seq_length)\n",
    "    \n",
    "    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
    "    return train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49feded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화\n",
    "# LayerNorm 가중치에는 가중치 감쇠 적용 X\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
    "            {'params': params_without_wd, 'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734121fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa07ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# 엑셀러레이트\n",
    "accelerator = Accelerator()\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "\n",
    "# 로깅\n",
    "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# 모델과 토크나이저를 로드합니다\n",
    "if accelerator.is_main_process:\n",
    "    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "# 데이터셋과 데이터로더를 로드합니다\n",
    "train_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n",
    "\n",
    "# 옵티마이저와 학습률 스케줄러를 준비합니다\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "                             num_warmup_steps=args.num_warmup_steps,\n",
    "                             num_training_steps=args.max_train_steps,)\n",
    "def get_lr():\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "# `accelerator`로 모든 것을 준비합니다(매개변수 순서는 중요하지 않습니다)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# 모델을 훈련합니다\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    loss = model(batch, labels=batch).loss\n",
    "    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n",
    "                       'steps': completed_steps, 'loss/train': loss.item()})\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    if step % args.save_checkpoint_steps == 0:\n",
    "        logger.info('Evaluating and saving model checkpoint')\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model.save_pretrained(\"./\")\n",
    "            hf_repo.push_to_hub(commit_message=f'step {step}')\n",
    "        model.train()\n",
    "    if completed_steps >= args.max_train_steps:\n",
    "        break\n",
    "\n",
    "# 마지막 체크포인트를 평가하고 저장합니다\n",
    "logger.info('Evaluating and saving model after training')\n",
    "eval_loss, perplexity = evaluate()\n",
    "log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model.save_pretrained(\"./\")\n",
    "    hf_repo.push_to_hub(commit_message=f'final model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db98181",
   "metadata": {},
   "source": [
    "### 결과 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "model_ckpt = 'transformersbook/codeparrot-small'\n",
    "generation = pipeline('text-generation', model=model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import set_seed \n",
    "\n",
    "def first_block(string):\n",
    "    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n",
    "\n",
    "def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n",
    "    set_seed(seed)\n",
    "    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n",
    "                  \"do_sample\":True,}\n",
    "    code_gens = generation(prompt, num_return_sequences=num_completions, \n",
    "                            max_length=max_length, **gen_kwargs)\n",
    "    code_strings = []\n",
    "    for code_gen in code_gens:\n",
    "        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n",
    "        code_strings.append(generated_code)\n",
    "    print(('\\n'+'='*80 + '\\n').join(code_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사각형 면적 계산\n",
    "prompt = '''def area_of_rectangle(a: float, b: float):\n",
    "    \"\"\"Return the area of the rectangle.\"\"\"'''\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab732ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML에서 URL 추출\n",
    "prompt = '''def get_urls_from_html(html):\n",
    "    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa7bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
