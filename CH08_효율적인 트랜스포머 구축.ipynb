{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0782056e",
   "metadata": {},
   "source": [
    "예측 속도를 높이고, 트랜스포머 모델의 메모리 사용량 줄이는 네 가지 기술  \n",
    ": 지식 정제 / 양자화 / 가지치기 / 그래프 최적화(ONNX 포맷과 ONNX 런타임 사용)\n",
    "\n",
    "### 의도 탐지 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c14cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c182c3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008950085ede45a0b623b7650b215240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knuyh\\.cache\\huggingface\\hub\\models--transformersbook--bert-base-uncased-finetuned-clinc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4470f172ed64875a888fd93612ea091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f628f17accc48f4b64047437382d448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e5be2d8fce4e7693669339f01d80b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8161e0b913e486b935b1ac5ca260b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4633ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.5490036010742188}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in\n",
    "Paris and I need a 15 passenger van\"\"\"\n",
    "pipe(query) # 예측한 의도와 신뢰도 점수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b134f2",
   "metadata": {},
   "source": [
    "### 벤치마크 클래스 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0099ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732f3330254b4e19aa5a430c6b3daa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/24.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0535ecc04e7d4e3fbc18d7ef1b5ee9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330dabeedbe2468c8bf67710c4a89adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/312k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c91036ea4d344bf8e17971ed8bfbb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42b2f17f6a9436a8865cd0a7e76ac64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/136k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5e38cecb034bb6a2dc1716c9bcf1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0480c7d0a7294925a7012108618ed852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e290860fd84690b109d98b659de167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b29ed9213a4832aa77102a68c9404d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\") # plus : 범위 밖의 훈련 샘플이 담긴 서브셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84e0758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = clinc[\"test\"][42]\n",
    "sample # 의도는 ID로 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc6ef04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transfer'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"]) # 의도 : ID를 문자열로 쉽게 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa09dc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c760333f3604b8289914cbf980a3f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_score = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d2c50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset :\n",
    "            pred = self.pipeline(example['text'])[0]['label']\n",
    "            label = example['intent']\n",
    "            preds.append(intents.str2int(pred))\n",
    "            labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions = preds, references = labels)\n",
    "        print(f\"테스트 세트 정확도 - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def compute_size(self):\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        # state_dict() : 모델의 층과 학습가능한 파라미터(가중치와 편향)를 매핑하는 dict 반환\n",
    "        # 키 : BERT 층 / 값 : BERT 텐서 \n",
    "        tmp_path = Path('model.pt')\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        # 메가바이트 단위로 크기 계산\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "        # 임시 파일 삭제\n",
    "        tmp_path.unlink()\n",
    "        print(f\"모델 크기 (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "        \n",
    "\n",
    "    def time_pipeline(self, query=\"What is the pin number for my account?\"):  # 쿼리마다 평균적인 레이턴시 잼\n",
    "        latencies = []\n",
    "        # 워밍업\n",
    "        for _ in range(10) :\n",
    "            _ = self.pipeline(query)\n",
    "        # 실행 측정\n",
    "        for _ in range(100) :\n",
    "            start_time = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        # 통계 계산\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"평균 레이턴시 (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "036afdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 크기 (MB) - 418.15\n",
      "평균 레이턴시 (ms) - 47.53 +\\- 5.07\n",
      "테스트 세트 정확도 - 0.867\n"
     ]
    }
   ],
   "source": [
    "# 기준 모델\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea55bd",
   "metadata": {},
   "source": [
    "### 지식 정제로 모델 크기 줄이기\n",
    "* 지식 정제 트레이너 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75c7dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha  # 정제 손실의 상대적인 가중치 제어\n",
    "        self.temperature = temperature # 레이블의 확률 분포를 얼마나 완만하게 만들지 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334cd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cecc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs = inputs.to(device)\n",
    "        outputs_stu = model(**inputs)\n",
    "        \n",
    "        # 스튜던트의 크로스 엔트로피 손실과 로짓 추출\n",
    "        loss_ce = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        \n",
    "        # 티처의 로짓 추출\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            logits_tea = outputs_tea.logits\n",
    "            \n",
    "        # 확률을 부드럽게하고 정제 손실 계산\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\") # KL 발산\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1))\n",
    "        \n",
    "        # 가중 평균된 스튜던트 손실 반환\n",
    "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "        return (loss, outputs_stu) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a913e",
   "metadata": {},
   "source": [
    "* 좋은 스튜던트 초기화 선택하기\n",
    "    * 레이턴시와 메모리 사용량을 줄이기 위해 스튜던트로 작은 모델 골라야\n",
    "    * 경험 법칙에 의하면 티처와 스튜던트가 동일한 종류의 모델일 때 지식 정제 잘 동작함 -- BERT와 RoBERTa처럼 모델 종류가 다를 때 출력 임베딩 공간이 달라 스튜던트가 티처를 모방하는 데 방해가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c241054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3c184455f74628b80ddcd01ebf7665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knuyh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\knuyh\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf7cdfad47440c1b508785446cbd3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3056dd4c55344428a4d98fa22c6308f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40edec21258145a384cd0a1b8b7ce48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a6c5d7ed1e461685456de671637660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd33a56a435455aac49a58c48e75289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8480fe944e4a7ea110c3867091aed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
    "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c72e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a37b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    output_dir='C:/Users/knuyh/Desktop', evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=3, learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01,\n",
    "    push_to_hub=True)\n",
    "# 처음에는 alpha를 1로 지정해 티처로부터 어떤 신호도 받지 않고 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f969fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_training_args.logging_steps = len(clinc_enc['train']) // batch_size\n",
    "student_training_args.disable_tqdm = False\n",
    "student_training_args.save_steps = 1e9\n",
    "student_training_args.log_level = 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3439a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c97e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "num_labels = intents.num_classes\n",
    "student_config = (AutoConfig\n",
    "                  .from_pretrained(student_ckpt, num_labels=num_labels,\n",
    "                                   id2label=id2label, label2id=label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19dd6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(student_ckpt, config=student_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3242321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_ckpt, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59538749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [235/954 24:48 < 1:16:33, 0.16 it/s, Epoch 0.74/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distilbert_trainer = DistillationTrainer(model_init=student_init,\n",
    "    teacher_model=teacher_model, args=student_training_args,\n",
    "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
    "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=finetuned_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_type = \"DistilBERT\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\n",
    "                        marker='$\\u25CC$')\n",
    "        else:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n",
    "\n",
    "    legend = plt.legend(bbox_to_anchor=(1,1))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80,90)\n",
    "    # 가장 느린 모델을 사용해 x 축 범위 지정\n",
    "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2f326",
   "metadata": {},
   "source": [
    "작은 모델을 사용해 평균 레이턴시를 크게 줄였음  \n",
    "티처의 정제 손실 추가해보기\n",
    "\n",
    "* 옵투나로 좋은 하이퍼파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23847c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -2, 2) # 균등하게 샘플링할 파라미터 범위 지정\n",
    "    y = trial.suggest_float(\"y\", -2, 2)\n",
    "    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9949fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params # 스터디 완료 후 최상의 파라미터 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371217d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86350d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = distilbert_trainer.hyperparameter_search(\n",
    "    n_trials=20, direction=\"maximize\", hp_space=hp_space)\n",
    "# 최대화된 목적 함수의 값과 하이퍼파라미터 담음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in best_run.hyperparameters.items():\n",
    "    setattr(student_training_args, k, v)\n",
    "\n",
    "# 정제된 모델을 저장할 새로운 저장소를 정의합니다\n",
    "distilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\n",
    "student_training_args.output_dir = distilled_ckpt\n",
    "\n",
    "# 최적의 매개변수로 새로운 Trainer 생성\n",
    "distil_trainer = DistillationTrainer(model_init=student_init,\n",
    "    teacher_model=teacher_model, args=student_training_args,\n",
    "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
    "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
    "\n",
    "distil_trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a34535",
   "metadata": {},
   "source": [
    "스튜던트의 매개변수 개수는 거의 티처의 절반이지만  \n",
    "티처의 정확도에 버금가는 스튜던트 훈련\n",
    "\n",
    "* 정제 모델 벤치마크 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=distilled_ckpt)\n",
    "optim_type = \"Distillation\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0563194",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7e1e1",
   "metadata": {},
   "source": [
    "모델 크기와 레이턴시는 기본적으로 DistilBERT에 비해 달라지지 않았지만, 정확도는 향상되고 티처의 성능도 뛰어난다.\n",
    "\n",
    "### 양자화로 모델 속도 높이기\n",
    "지식 정제를 사용해 티처의 정보를 작은 스튜던트 모델로 전송해 추론 실행 시, 계산 비용과 메모리 사용량 줄이는 방법  \n",
    "\n",
    "<br>\n",
    "양자화 방식 : 계산량을 줄이는 대신 가중치와 활성화 출력을 32비트 부동 소수점(FP32)이 아닌 8비트 정수(INT8) 같이 정밀도가 낮은 데이터 타입으로 변환해 계산 효율적으로 수행\n",
    "\n",
    "* 아핀 변환 : 고정 소수점 숫자를 역양자화해 부동 소수점으로 되돌려야 하는 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edaf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = pipe.model.state_dict()\n",
    "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
    "plt.hist(weights.flatten().numpy(), bins=250, range=(-0.3,0.3), edgecolor=\"C0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6019aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_point = 0\n",
    "scale = (weights.max() - weights.min()) / (127 - (-128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52526004",
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights / scale + zero_point).clamp(-128, 127).round().char()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caecd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import quantize_per_tensor\n",
    "\n",
    "dtype = torch.qint8\n",
    "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\n",
    "quantized_weights.int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8dc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 가중치에서 양자화 효과\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes,mark_inset\n",
    "\n",
    "# 히스토그램 그리기\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(quantized_weights.dequantize().flatten().numpy(),\n",
    "         bins=250, range=(-0.3,0.3), edgecolor=\"C0\");\n",
    "# 확대 그림 만들기\n",
    "axins = zoomed_inset_axes(ax, 5, loc='upper right')\n",
    "axins.hist(quantized_weights.dequantize().flatten().numpy(),\n",
    "         bins=250, range=(-0.3,0.3));\n",
    "x1, x2, y1, y2 = 0.05, 0.1, 500, 2500\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "axins.axes.xaxis.set_visible(False)\n",
    "axins.axes.yaxis.set_visible(False)\n",
    "mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bca9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weights @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce75d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.quantized import QFunctional\n",
    "\n",
    "q_fn = QFunctional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96843797",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "q_fn.mul(quantized_weights, quantized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f471474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 텐서와 양자화된 텐서 저장크기 비교\n",
    "import sys\n",
    "\n",
    "sys.getsizeof(weights.untyped_storage()) / sys.getsizeof(quantized_weights.untyped_storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e1555",
   "metadata": {},
   "source": [
    "보통 선형 층만 양자화한다.  \n",
    "양자화에서 주의할 점 - 모델에 있는 모든 연산에서 정밀도를 바꾸면 모델의 계산 그래프 각 지점에서 작은 변동 생김\n",
    "<br>\n",
    "\n",
    "[심층 신경망에서 사용하는 양자화 방법]\n",
    "- 동적 양자화\n",
    "    * 훈련 도중에 아무것도 바뀌지 않고 추론 과정에만 적응\n",
    "    * 모델 가중치가 추론 전에 INT8로 변환\n",
    "    * 가중치 외에 모델의 활성화도 양자화되는데, 이 양자화가 즉석에서 일어남\n",
    "    * 부동 소수점 포맷으로 활성화를 메모리에 쓰고 읽어 정수와 부동 소수점 간의 변환이 성능 병목이 되는 경우도 있음\n",
    "    \n",
    "- 정적 양자화\n",
    "    * 즉석에서 활성화를 양자화하지 않고 양자화 체계를 사전에 계산해 부동 소수점 변환을 피함\n",
    "    * 추론에 앞서 대표 샘플 데이터에서 활성화 패턴을 관찰해 수행한 후 이상적인 양자화 체계를 계산해 저장\n",
    "    * 훈련과 추론 과정에서 정밀도 차이로 모델 성능이 떨어짐\n",
    "    \n",
    "- 양자화를 고려한 훈련\n",
    "    * 가짜로 FP32 값을 양자화해 훈련 중에 양자화의 효과 시뮬레이션\n",
    "    * 훈련할 때 INT8 대신 FP32 반올림해 양자화 효과 흉내냄\n",
    "    * 정방향 패스와 역방향 패스에서 모두 적용되며 정적 양자화와 동적 양자화 사용해 모델 성능 향상시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb006f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 양자화\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt).to(\"cpu\"))\n",
    "\n",
    "model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76362741",
   "metadata": {},
   "source": [
    "모델 정확도에 미치는 영향 거의 없음 (양자화 특징)\n",
    "\n",
    "### 양자화된 모델의 벤치마크 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88badc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model_quantized,\n",
    "                tokenizer=tokenizer)\n",
    "optim_type = \"Distillation + quantization\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c9f4c",
   "metadata": {},
   "source": [
    "양자화된 모델의 크기가 정제된 모델의 거의 절반이고 심지어 성능도 약간 향상됐다.\n",
    "\n",
    "### ONNX와 ONNX 런타임으로 추론 최적화하기\n",
    "* ONNX는 변경 불가능한 연산 규격을 그룹화하기 위해 연산자 집합 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42000674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from psutil import cpu_count\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\" # 대기 스레드 활성 상태로 지정(CPU프로세서 사이클 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520db58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
    "onnx_model_path = Path(\"onnx/model.onnx\")\n",
    "convert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\n",
    "        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d339aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import (GraphOptimizationLevel, InferenceSession,\n",
    "                         SessionOptions)\n",
    "\n",
    "def create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = create_model_for_provider(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = clinc_enc[\"test\"][:1]\n",
    "del inputs[\"labels\"]\n",
    "logits_onnx = onnx_model.run(None, inputs)[0]\n",
    "logits_onnx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(logits_onnx) # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinc_enc[\"test\"][0][\"labels\"] # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db09988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class OnnxPipeline:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, query):\n",
    "        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n",
    "        inputs_onnx = {k: v.cpu().detach().numpy()\n",
    "                       for k, v in model_inputs.items()}\n",
    "        logits = self.model.run(None, inputs_onnx)[0][0, :]\n",
    "        probs = softmax(logits)\n",
    "        pred_idx = np.argmax(probs).item()\n",
    "        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3988ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OnnxPipeline(onnx_model, tokenizer)\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03115dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxPerformanceBenchmark(PerformanceBenchmark):\n",
    "    def __init__(self, *args, model_path, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def compute_size(self):\n",
    "        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"모델 크기 (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadf6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_type = \"Distillation + ORT\"\n",
    "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n",
    "                              model_path=\"onnx/model.onnx\")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0780ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b22aae",
   "metadata": {},
   "source": [
    "레이턴시 향상됨\n",
    "\n",
    "ORT모델을 양자화하기 위해 세 가지 방법 제공  \n",
    "이 중 정제 모델에 동적 양자화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_input = \"onnx/model.onnx\"\n",
    "model_output = \"onnx/model.quant.onnx\"\n",
    "quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b56b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_quantized_model = create_model_for_provider(model_output)\n",
    "pipe = OnnxPipeline(onnx_quantized_model, tokenizer)\n",
    "optim_type = \"Distillation + ORT (quantized)\"\n",
    "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type,\n",
    "                              model_path=model_output)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c179ca9",
   "metadata": {},
   "source": [
    "ORT 양자화는 일반 양자화로 얻은 모델(Distillation+quantization)에 비해 모델 크기와 레이턴시 30% 가량 줄임  \n",
    "ONNX는 임베딩 층도 양자화하기 때문\n",
    "\n",
    "### 가중치 가지치기로 희소한 모델 만들기\n",
    "양자화 같은 방법은 표현 정밀도를 낮춰 모델 크기를 줄인다.  \n",
    "* 가중치 가지치기 : 일부 가중치를 제거해 크기를 줄이는 전략\n",
    "    * 절댓값 가지치기 : 가중치 절댓값 크기에 따라 점수 계산\n",
    "        * 계산량이 많다.\n",
    "        * 두 개의 가중치 클러스터 만듦\n",
    "        * 각 가중치의 중요도가 현재 작업과 직접적으로 관련된 순수한 지도 학습을 위해 고안됐다는 문제\n",
    "        * 미세 튜닝 작업에서 중요한 가중치 삭제될 가능성\n",
    "    * 이동 가지치기 : 미세 튜닝하는 동안 점진적으로 가중치 제거해 모델을 점차 희소하게 만듦\n",
    "        * 미세 튜닝동안 가중치와 점수 모두 학습됨\n",
    "        * 완만한 분포 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 절댓값 가지치기에 사용되는 세제곱 희소성 스케줄러\n",
    "def _sparsity(t, t_0=0, dt=1, s_i=0, s_f=0.9, N=100):\n",
    "    return s_f + (s_i - s_f) * (1 - (t - t_0) / (N * dt))**3\n",
    "\n",
    "steps = np.linspace(0,100,100)\n",
    "values = [_sparsity(t) for t in steps]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(steps, values)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_xlabel(\"Pruning step\")\n",
    "ax.set_ylabel(\"Sparsity\")\n",
    "plt.grid(linestyle=\"dashed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211dd7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
